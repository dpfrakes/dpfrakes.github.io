---
title: "Machine Learning Hasn't Failed Us, We Have"
date: 
draft: true
tags: ["ethics", "machine learning", "society"]
---

I was watching a presentation by QuantumBlack, a technology company touting
itself as an advocate of "augmented intelligence" instead of "artificial
intelligence," at the Royal Institution ([link](https://youtu.be/JmUFAGgKqjs)).
In it, Martha Imprialou talks about some of the tragedies of poorly-executed
implementations of machine learning in our world with a slide titled "When
machine learning fails to deliver."

![When Machine Learning fails to deliver](../_resources/when-ml-fails-to-deliver.png)

The three examples given were:

1. A self-driving Uber car hit and killed a pedestrian
2. Amazon's one-day delivery service area excluded ~50% of the black population
in Chicago
3. Google's sentiment analyzer rated the terms "Jew" and "homosexual"
negatively

All three of these incidents are, at best, unfortunate and, at worst,
tragedies.

I have a problem, however, with how machine learning, a term used to label a
family of mathematical algorithms, was presented as the guilty party
responsible for negligence and racism.

First of all, technology of any kind is not our savior. Never has been, never
will be.

Second, AI (call it "artificial" or "augmented," I don't care about your
marketing) is a tool just like many other tools invented in human history that
was designed to, and does, increase the efficiency of already-human activity.
Racism, violence, and negligence are as old as humankind. It took exactly one
generation of humanity before murder was committed.

Microsoft's first Twitter bot, Tay, was a PR disaster. Not because the
technologists who built it were malicious or racist, but because of the
human interactions that Tay experienced: trolls and racists on the web.

The Xbox Kinect, one of the first high-fidelity video game console camera
systems, was declared "racist" by the populous because it struggled to
recognize people with dark skin tones. It's a camera, not Jim Crow.

Amazon has no interest in your skin color, only profit. Anyone who thinks this
behemoth of Big Data didn't do their homework before deciding where to offer
their premium services hasn't done enough thinking.

Google doesn't care about your feelings, it just wants its algorithms to play
the part of humans well enough to offload tedious tasks onto cheap robots so
they can free up their expensive humans to tackle harder problems. If enough
humans are racist (and there are plenty), then the algorithms that output
racist results are doing their job _until_ they are told to censor a group of
certain humans.

Don't get me wrong, I don't like Big Tech. But don't blame math for humanity's
shortcomings.

P.S. I do like the idea of "augmented" as opposed to "artificial" intelligence,
though it is just marketing. Can we not portray this as cyborgs though? I get
that PowerPoint didn't exist when the iron plow was invented, and it's tough to
visualize a support vector machine in a summary slide, but displaying a person
whose body parts have been amputated and replaced with hardware marketed as
near-sentient isn't going to help increase technological adoption. Thanks.
