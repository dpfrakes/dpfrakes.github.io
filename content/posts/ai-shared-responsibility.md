---
title: "AI Shared Responsibility"
date: 2022-02-02T00:00:00-04:00
draft: false
tags: [work]
---

Who's to blame when an AI-driven tool does something bad?

VW GTI diesel scandal: executive blamed a "rogue engineer"

MIT Moral Machine

Lots of complaints about billionaires profiting off their employees (what boss
doesn't, btw?) and not taking enough responsibility while fostering a false
"when you're here, you're family" work environment.

Let's keep AI a team sport.

Racial bias is all over the news nowadays. While we should be reporting on the
incredible potential for AI to address global-scale problems, we're instead
trying to fix false positives in facial recognition software that
misidentifies black people more often than white people. It's a big problem,
but who are we blaming? Is it the developers? The designers? Data scientists?
Executives?

This problem stems from a lack of sufficient data. Specifically, a lack of
facial images of black people. But if a company starts gathering more images
of black people's faces, they'll get crucified for disproportionately
gathering and storing PII data of black people.

Luckily there's a solution: GANs. Make up synthetic people.

## it's about culture

Culture is hard to build/foster/repair within a work environment

It's even harder to do so for the general public of a nation state.

But if people have an expectation
